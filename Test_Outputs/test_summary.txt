MNIST Neural Network Test Summary
========================================

Network Architecture:
- Input: 784 pixels (28x28 flattened) - Q1.7 format
- Layer L0: 16 neurons (ReLU) - Weights/Biases in Q6.10 format
- Layer L1: 10 neurons (Softmax) - Weights/Biases in Q6.10 format
- Outputs: Q1.7 format

Fixed-Point Formats:
- Q1.7: 8-bit, range -1.0 to +0.9921875 (inputs/outputs)
- Q6.10: 16-bit, range -32.0 to +31.999023438 (weights/biases)

Test Results:
Accuracy on 10 samples: 90.0%

Sample-by-Sample Results:
Sample 0: True=7, Predicted=7, Confidence=0.9912
Sample 1: True=2, Predicted=2, Confidence=0.9996
Sample 2: True=1, Predicted=1, Confidence=0.9765
Sample 3: True=0, Predicted=0, Confidence=1.0000
Sample 4: True=4, Predicted=4, Confidence=0.9831
Sample 5: True=1, Predicted=1, Confidence=0.9949
Sample 6: True=4, Predicted=4, Confidence=0.9916
Sample 7: True=9, Predicted=9, Confidence=0.9939
Sample 8: True=5, Predicted=6, Confidence=0.8397
Sample 9: True=9, Predicted=9, Confidence=0.8884

File Structure:
- Weights_Biases/: Neural network parameters in Q6.10 format (16-bit)
- inputs/: Test input images in Q1.7 format (8-bit)
- layer_outputs/: L0 hidden layer outputs in Q1.7 format
- final_outputs/: L1 output layer (softmax) outputs in Q1.7 format
- Individual neuron files contain decimal, binary, and hex values
